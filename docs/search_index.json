[["index.html", "Instrumental Phonetics 2022 1 About the course 1.1 Recording in case you missed the class 1.2 Homeworks 1.3 Useful references:", " Instrumental Phonetics 2022 G. Moroz 1 About the course \\[ \\text{Final Grade} = 0.6 \\times \\text{HW} + 0.4 \\times \\text{Exam} \\] I expect some theoretical knowledge read 2. chapter from (Gussenhoven and Jacobs 2017) see the presentation on Phon research from 13.09.2021 be able to use IPA symbols I expect some basic R skills: import .csv files to R dplyr, ggplot2 1.1 Recording in case you missed the class on acoustics on Praat and Biocoustics on Vowels and duration extraction on sonorants 1.2 Homeworks 1.2.1 HW 1 (due to 23.02.2022): Create an annotation for those sound file containing words mza ‘lamp’, mzə ‘moon’, mtsa ‘fire’, mtsə ‘lie’. Here is an example: Extract vowel duration and formants and plot them: Send the result .TextGrid and your .R code files to the course assistant Anya Tsyzova with the topic Instrumental Phonetics: HW 1. 1.3 Useful references: (Ashby and Maidment 2005) (Bickford and Floyd 2006) (Boersma and Weenink 2021) (Fant 1960a) (Fuchs, Toda, and Żygis 2010) (Fulop 2011) (Gick, Wilson, and Derrick 2012) (Gordon and Ladefoged 2001) (Harrington 2010) (Hunt 2009) (Johnson 2004) (Ladefoged and Disner 2012) (Maddieson and Ladefoged 1996) (Rorabaugh 2010) (Grama 2022) (Fridland and Kendall 2022) References "],["introduction-to-acoustic-phonetics.html", "2 Introduction to Acoustic Phonetics 2.1 Before we start 2.2 Phonetics?.. 2.3 Simple Harmonic Motion 2.4 Addition of waves 2.5 Fourier Transform 2.6 Source-Filter Model of Speech Production 2.7 Summary", " 2 Introduction to Acoustic Phonetics 2.1 Before we start If you missed the class, please see the recording from the previous year. 2.2 Phonetics?.. Phonetics is generally assumed to be a subfield that deals with articulatory, acoustic and perceptional aspects of phonological units. Phonology and phonetics together are supposed to describe organization of sounds in languages. This course is about acoustic phonetics. 2.3 Simple Harmonic Motion Periodic Motion is any type of motion that repeats itself after successuve equal time intervals. Simple Harmonic Motion is specific type of periodic motion that arises from existence of some equilibrium position for a described object; linear restoring force that tending to pull the described object back to its equilibrium position. There are several parameters of wave: Amplitude (A) is the maximum displacement of the equilibrium position. Period (T) is the duration of time of one cycle in a repeating event. Measured in seconds. Frequency (f) is the number of period (cycles) per second. Measured in Hz. \\[ f = \\frac{1}{T} \\] \\[ T = \\frac{1}{f} \\] We can correlate the physical properties of sound waves with our perception: We perceive changes in frequency as pitch; We perceive changes in amplitude as loudness. One period of SHM can be devided into 360° of phase φ. So for the case of two SHM they can be out of phase: … or one way can be 90° ahead: So after all we have everything important for the wave definition: \\[ s(t) = A \\times \\cos(2\\pi ft + \\phi) \\] A — amplitude; f — is the fundamental frequency; φ — phase; t — time. 2.4 Addition of waves If we add some waves, we will get the new wave: Beats — beats is a phenomenon of the change in amplitude of the sum of two waves with slightly different frequencies. Here is an example from Wikipedia. 2.5 Fourier Transform Fourier Transform allows to extract components of the complex wave. smoothie complex wave ↓ ↓ 1 banana, cut in chunks 300 Hz 1 cup grapes 1000 Hz vanilla yogurt 1/2 apple, cored and chopped 1.5 cup fresh spinach leaves Spectrograms are differ in window length: Syllable [ka] Conventional spectrogram and Zhao-Atlas-Marks distribution of the English word had, computed using a Kaiser tapering function (Fulop 2011: 119): Conventional and reassigned spectrograms of the English word right (Fulop 2011: 42): 2.6 Source-Filter Model of Speech Production The output energy (at the mouth) for a given frequency is equal to the amplitude the source harmonic, multiplied by the magnitude of the filter function for that the frequency. 2.7 Summary sounds are waves (with amplitude, frequency and phase) simple waves can be combined to the complex one Fourier transform allows to extract components of the complex wave It is not only Fourier transform that allows to extract components of the complex wave Source-Filter Model: vocal tract is a resonator that filters some frequencies of the wave produced by vocal folds vibration. References "],["animal-bioacoustics-based-on-fletcher07.html", "3 Animal bioacoustics based on (Fletcher 2007) 3.1 Some facts 3.2 Hearing and Sound Production 3.3 Vibrational Communication 3.4 Insects 3.5 Land Vertebrates 3.6 Birds 3.7 Bats 3.8 Aquatic Animals", " 3 Animal bioacoustics based on (Fletcher 2007) Bioacoustics — is a subbranch of biology focused on sound production, dispersion and reception in animals (including humans). 3.1 Some facts there is a relation between mass and frequency range: patterns are easier than human patterns could be really fast patterns could be really slow some animals use some tricks! BBC, Earth’s tropical Island – Borneo (20:47) animals adapt their frequency range to environment and to each other (environment pollution, listen to NPR Invisibilia’s episode The Last Sound) 3.2 Hearing and Sound Production Hearing is surprisingly simmilar Sound production of breathing animals: non-aquatic mammals exhalation through valve aquatic mammals moving are from one reservoir to another through the oscillating valve Sound production of non-breathing animals: muscle-driven mechanical vibrations 3.3 Vibrational Communication some animals communicate through vibration some animals do both: Владимир Динец (2015) Песни драконов 3.4 Insects external sensory hairs ribbed file on their legs, or wings some crickets have evolved the strategy of digging a horn-shaped burrow in the earth Tenrecs! 3.5 Land Vertebrates 3.5.1 Land vertebrates Some animals adjust their vocal system so that the frequency of the vocal valve closely matches a major resonance of the upper vocal tract, usually that of lowest frequency but not necessarily so. Some species of frogs and birds achieve this by the incorporation of an inflatable sac in the upper vocal tract. (cf. air sacs in apes, e. g. (Hewitt, MacLarnon, and Jones 2002)) Some animals change their frequency range according to environment In most mammals and other large animals the auditory canal joining the two ears in birds and reptiles has generally degenerated in mammals to the extent that each ear functions nearly independently. 3.6 Birds Song birds have a syrinx consisting of dual inflated-membrane valves. These valves can be operated simultaneously and sometimes at different frequencies (see overtone singing), but more usually separately, and produce a pulsating air-flow rich in harmonics. Some birds have developed the ability to mimic others around them BTW: here is the database of bird sounds 3.7 Bats echo-location (cf. blind or visually impaired people) short calls huge range 40-80 kHz sound emitted through the nose rather than the mouth 3.8 Aquatic Animals crustaceans – like insects – produce sound by rubbing a toothed leg-file against one of the plates covering their body fish species with swim-bladder – like other insects – membrane over the bladder, that oscilates by muscular effort different system of hearing (hair-cells, otolith) BTW: check out a Hydrophone References "],["vowels.html", "4 Vowels 4.1 Theory", " 4 Vowels 4.1 Theory 4.1.1 Recap Sound waves can be described as \\[ s(t) = A \\times \\cos(2\\pi ft + \\phi) \\] A — amplitude; f — is the fundamental frequency; φ — phase; t — time. Speech sounds are complex waves Fourier transform — allows to extract components of the complex wave Larynx produce some sound Vocal tract filter some frequencies 4.1.2 How shape of the vocal tract influences on vowels? Tube model. Historically, height and backness are impressionistic linguistic terms: But we are intersted just in a cardinal points: If we analyze acoustics we can get something like this: i a u F1 300 700 300 F2 2300 1400 800 However, if we analyze real sounds it could be messy: Tube model, after (Fant 1960b): vocal tract is a tube or a set of tubes: 4.1.3 Wavelength \\[c = \\frac{\\lambda}{T} = \\lambda\\times f \\approx 33400\\text{ cm/s}\\] c — speed of sound; λ — wavelength; f — sound frequency; T — period. Neutral vocal tract in the position for the vowel ə: Resonance is a phenomenon in which a vibrating system or external force drives another system to oscillate with greater amplitude at specific frequencies. The lowest natural frequency at which such a tube resonates will have a wavelength (λ) four times the length of the tube (L). \\[c = \\frac{\\lambda}{T} = \\lambda\\times f \\approx 33400\\text{ cm/s}\\] The tube also resonates at odd multiples of that frequency. \\[F_1 = \\frac{c}{\\lambda} = \\frac{c}{4 \\times L} \\approx 500 \\text{ Hz}\\] \\[F_2 = \\frac{c}{\\lambda} = \\frac{c}{\\frac{4}{3} \\times L} = \\frac{3 \\times c}{4 L} \\approx 1500 \\text{ Hz}\\] \\[F_3 = \\frac{c}{\\lambda} = \\frac{c}{\\frac{4}{5} \\times L} = \\frac{5 \\times c}{4 L} \\approx 2500 \\text{ Hz}\\] \\[F_n = \\frac{c}{\\lambda} = \\frac{c}{\\frac{4}{n} \\times L} = \\frac{n \\times c}{4 L} \\approx n \\times 500 \\text{ Hz}\\] Something like this we can expect from animals: When there is a constriction, back tube and constriction form Helmholtz resonator. \\[f = \\frac{c}{2\\pi} \\times \\sqrt{\\frac{A}{V\\times L}}\\] A — the area of the neck; L — length of the tube; V — volume of the air in the body. 4.1.4 Other models Perturbation Theory [Kajiyama 1941, Mrayati et al. 1988] Quantal Theory (Stevens 1972) Theory of adaptive dispersion (Lindblom and Maddieson 1988) References "],["r-packages-for-phonetics.html", "5 R packages for phonetics 5.1 phonfieldwork 5.2 ipa 5.3 phonTools 5.4 vowels 5.5 rPraat 5.6 speakr", " 5 R packages for phonetics 5.1 phonfieldwork We will work with examples from Abaza and will try to end up with something like this: 5.1.1 Befor we start I expect you to install tidyverse and phonfieldwork: install.packages(c(&quot;tidyverse&quot;, &quot;rmarkdown&quot;, &quot;phonfieldwork&quot;)) load the library: library(phonfieldwork) I will use the following version of the package: packageVersion(&quot;phonfieldwork&quot;) [1] &#39;0.0.12&#39; download files for todays seminar: first file second file 5.1.2 Philosophy of the phonfieldwork package Most phonetic research consists of the following steps: Formulate a research question. Think of what kind of data is necessary to answer this question, what is the appropriate amount of data, what kind of annotation you will do, what kind of statistical models and visualizations you will use, etc. Create a list of stimuli. Elicite list of stimuli from speakers who signed an Informed Consent statement, agreeing to participate in the experiment to be recorded on audio and/or video. Keep an eye on recording settings: sampling rate, resolution (bit), and number of channels should be the same across all recordings. Annotate the collected data. Extract the collected data. Create visualizations and evaluate your statistical models. Report your results. Publish your data. The phonfieldwork package is created for helping with items 3, partially with 4, and 5 and 8. To make the automatic annotation of data easier, I usually record each stimulus as a separate file. While recording, I carefully listen to my consultants to make sure that they are producing the kind of speech I want: three isolated pronunciations of the same stimulus, separated by a pause and contained in a carrier phrase. In case a speaker does not produce three clear repetitions, I ask them to repeat the task, so that as a result of my fieldwork session I will have: a collection of small soundfiles (video) with the same sampling rate, resolution (bit), and number of channels a list of succesful and unsuccesful attempts to produce a stimulus according to my requirements (usually I keep this list in a regular notebook) 5.1.3 Make a list of your stimuli First we need to create a list of stimuli. We want to record two Abaza words from speakers (in real life word lists are much longer). my_stimuli_df &lt;- read.csv(&quot;https://raw.githubusercontent.com/agricolamz/2022_HSE_m_Instrumental_phonetics/master/data/my_stimuli_df.csv&quot;) my_stimuli_df It is also possible to store your list as a column in an .xls or xlsx file and read it into R using the read_xls or read_xlsx functions from the readxl package. If the package readxl is not installed on your computer, you can install it using install.packages(\"readxl\"). 5.1.4 Create a presentation based on a list of stimuli You can show a native speaker your stimuli one by one or not show them the stimuli but ask them to pronounce a certain stimulus or its translation. I use presentations to collect all stimuli in a particular order without the risk of omissions. When the list of stimuli is loaded into R, you can create a presentation for elicitation. It is important to define an output directory, so in the following example I use the getwd() function, which returns the path to the current working directory. You can set any directory as your current one using the setwd() function. It is also possible to provide a path to your intended output directory with output_dir (e. g. “/home/user_name/…”). This command (unlike setwd()) does not change your working directory. create_presentation(stimuli = my_stimuli_df$stimuli, output_file = &quot;first_example&quot;, output_dir = &quot;sounds/&quot;) As a result, a file “first_example.html” was created in the output folder. You can change the name of this file by changing the output_file argument. The .html file now looks as follows: https://agricolamz.github.io/2022_HSE_m_Instrumental_phonetics/additional/first_example.html It is also possible to change the output format, using the output_format argument. By dafault it is “html,” but you can also use “pptx.” There is also an additional argument translations, where you can provide translations for stimuli in order that they appeared near the stimuli on the slide. create_presentation(stimuli = my_stimuli_df$stimuli, translations = my_stimuli_df$translation, output_file = &quot;second_example&quot;, output_dir = &quot;sounds/&quot;) https://agricolamz.github.io/2022_HSE_m_Instrumental_phonetics/additional/second_example.html 5.1.5 Rename collected data After collecting data and removing soundfiles with unsuccesful elicitations, one could end up with the following structure: sounds ├── 01.wav └── 02.wav rename_soundfiles(stimuli = my_stimuli_df$stimuli, prefix = &quot;s1_&quot;, path = &quot;sounds/examples/&quot;) You can find change correspondences in the following file: /home/agricolamz/work/materials/2022_HSE_m_instrumental_phonetics/materials/sounds/examples/backup/logging.csv As a result, you obtain the following structure: sounds ├── 1_s1_ba.wav ├── 2_s1_bzə.wav └── backup ├── 01.wav ├── 02.wav └── logging.csv The rename_soundfiles() function created a backup folder with all of the unrenamed files, and renamed all files using the prefix provided in the prefix argument. There is an additional argument backup that can be set to FALSE (it is TRUE by default), in case you are sure that the renaming function will work properly with your files and stimuli, and you do not need a backup of the unrenamed files. There is also an additional argument logging (TRUE by default) that creates a logging.csv file in the backup folder (or in the original folder if the backup argument has value FALSE) with the correspondences between old and new names of the files. Here is the contence of the logging.csv: 5.1.6 Merge all data together After all the files are renamed, you can merge them into one. Remmber that sampling rate, resolution (bit), and number of channels should be the same across all recordings. It is possible to resample files with the resample() function from the package biacoustics. concatenate_soundfiles(path = &quot;sounds/examples/&quot;, result_file_name = &quot;s1_all&quot;) This comand creates a new soundfile s1_all.wav and an asociated Praat TextGrid s1_all.TextGrid: ├── 1_s1_ba.wav ├── 2_s1_bzə.wav ├── s1_all.TextGrid ├── s1_all.wav └── backup ├── 01.wav ├── 02.wav └── logging.csv The resulting file can be parsed with Praat: Sometimes recorded sounds do not have any silence at the beginning or the end, so after the merging the result utterances will too close to each other. It is possible to fix using the argument separate_duration of the concatenate_soundfiles() function: just put the desired duration of the separator in seconds. It is not kind of task that could occur within phonfieldwork philosophy, but it also possible to merge multiple .TextGrids with the same tier structure using concatente_textgrids() function. 5.1.7 Annotate your data It is possible to annotate words using an existing annotation: my_stimuli_df$stimuli [1] &quot;ba&quot; &quot;bzə&quot; annotate_textgrid(annotation = my_stimuli_df$stimuli, textgrid = &quot;sounds/examples/s1_all.TextGrid&quot;) As you can see in the example, the annotate_textgrid() function creates a backup of the tier and adds a new tier on top of the previous one. It is possible to prevent the function from doing so by setting the backup argument to FALSE. annotate_textgrid(annotation = my_stimuli_df$translation, textgrid = &quot;sounds/examples/s1_all.TextGrid&quot;, tier = 2, backup = FALSE) Imagine that we are interested in annotation of vowels. The most common solution will be open Praat and create new annotations. But it is also possible to create them in advance using subannotations. The idea that you choose some baseline tier that later will be automatically cutted into smaller pieces on the other tier. create_subannotation(textgrid = &quot;sounds/examples/s1_all.TextGrid&quot;, tier = 1, # this is a baseline tier n_of_annotations = 9) # how many empty annotations per unit? It is worth mentioning that if you want to have different number of subannotation per unit, you can pass a vector of required numbers to n_of_annotations argument. After the creation of subannotations, we can annotate created tier: annotate_textgrid(annotation = c(&quot;&quot;, &quot;a&quot;, &quot;&quot;, &quot;a&quot;, &quot;&quot;, &quot;a&quot;, &quot;&quot;, &quot;a&quot;, &quot;&quot;, &quot;&quot;, &quot;ə&quot;, &quot;&quot;, &quot;ə&quot;, &quot;&quot;, &quot;ə&quot;, &quot;&quot;, &quot;ə&quot;, &quot;&quot;), textgrid = &quot;sounds/examples/s1_all.TextGrid&quot;, tier = 3, backup = FALSE) We can also add some tier for uterance annotation: create_subannotation(textgrid = &quot;sounds/examples/s1_all.TextGrid&quot;, tier = 1, n_of_annotations = 9) annotate_textgrid(annotation = c(&quot;&quot;, &quot;u_1&quot;, &quot;&quot;, &quot;u_2&quot;, &quot;&quot;, &quot;u_3&quot;, &quot;&quot;, &quot;cf&quot;, &quot;&quot;, &quot;&quot;, &quot;u_1&quot;, &quot;&quot;, &quot;u_2&quot;, &quot;&quot;, &quot;u_3&quot;, &quot;&quot;, &quot;cf&quot;, &quot;&quot;), textgrid = &quot;sounds/examples/s1_all.TextGrid&quot;, tier = 4, backup = FALSE) You can see that we created a third tier with annotation. The only thing left is to move annotation boundaries in Praat (this can not be automated): Here you can download the result .TextGrid (you can press Ctrl+S, when you open it). 5.1.8 Extracting your data First, it is important to create a folder where all of the extracted files will be stored: dir.create(&quot;sounds/examples/s1_sounds&quot;) It is possible to extract all annotated files based on an annotation tier: extract_intervals(file_name = &quot;sounds/examples/s1_all.wav&quot;, textgrid = &quot;sounds/examples/s1_all.TextGrid&quot;, tier = 3, path = &quot;sounds/examples/s1_sounds/&quot;, prefix = &quot;s1_&quot;) sounds/examples ├── 1_s1_ba.wav ├── 2_s1_bzə.wav ├── s1_all.TextGrid ├── s1_all.wav ├── backup │ ├── 01.wav │ ├── 02.wav │ └── logging.csv └── s1_sounds ├── 01_s1_a.wav ├── 02_s1_a.wav ├── 03_s1_a.wav ├── 04_s1_a.wav ├── 05_s1_ə.wav ├── 06_s1_ə.wav ├── 07_s1_ə.wav └── 08_s1_ə.wav 5.1.9 Visualizing your data It is possible to view an oscilogram and spetrogram of any soundfile: draw_sound(file_name = &quot;sounds/examples/s1_sounds/01_s1_a.wav&quot;) There are additional parameters: title – the title for the plot from – time in seconds at which to start extraction to – time in seconds at which to stop extraction zoom – time in seconds for zooming spectrogram text_size – size of the text on the plot annotation – the optional file with the TextGrid’s file path or dataframe with annotations (see the section 5.) freq_scale – the measure of the frequency: can be “Hz” or “kHz.” frequency_range – the frequency range to be displayed for the spectrogram dynamic_range – values greater than this many dB below the maximum will be displayed in the same color window_length – the desired length in milliseconds for the analysis window window – window type (can be “rectangular,” “hann,” “hamming,” “cosine,” “bartlett,” “gaussian,” and “kaiser”) preemphasisf – Preemphasis of 6 dB per octave is added to frequencies above the specified frequency. For no preemphasis (important for bioacoustics), set to a 0. spectrum_info – logical value, if FALSE won’t print information about spectorgram on the right side of the plot. output_file – the name of the output file output_width – the width of the device output_height – the height of the device output_units – the units in which height and width are given. This can be “px” (pixels, which is the default value), “in” (inches), “cm” or “mm.” It is really important in case you have a long file not to draw the whole file, since it won’t fit into the RAM of your computer. So you can use from and to arguments in order to plot the fragment of the sound and annotation: draw_sound(&quot;sounds/examples/s1_all.wav&quot;, &quot;sounds/examples/s1_all.TextGrid&quot;) 5.1.10 Read linguistic files into R The phonfieldwork package provides also several methods for reading different file types into R. This makes it possible to analyze them and convert into .csv files (e. g. using the write.csv() function). The main advantage of using those functions is that all of them return data.frames with columns (time_start, time_end, content and source). This make it easer to use the result in the draw_sound() function that make it possible to visualise all kind of sound annotation systems. file .TextGrid from Praat; see also rPraat and textgRid packages df &lt;- textgrid_to_df(&quot;sounds/examples/s1_all.TextGrid&quot;) df library(tidyverse) df %&gt;% filter(tier_name == &quot;vowels&quot; | tier_name == &quot;uterances&quot;, content != &quot;&quot;) %&gt;% mutate(duration = time_end - time_start) %&gt;% select(content, tier_name, duration) %&gt;% pivot_wider(names_from = tier_name, values_from = content) -&gt; results results %&gt;% ggplot(aes(vowels, duration))+ geom_point() 5.1.11 Documentation You can find the whole documentation for phonfieldwork here. If you will use the package it make sense to cite it: citation(&quot;phonfieldwork&quot;) Moroz G (2020). _Phonetic fieldwork and experiments with phonfieldwork package_. &lt;URL: https://CRAN.R-project.org/package=phonfieldwork&gt;. A BibTeX entry for LaTeX users is @Manual{, title = {Phonetic fieldwork and experiments with phonfieldwork package}, author = {George Moroz}, year = {2020}, url = {https://CRAN.R-project.org/package=phonfieldwork}, } 5.2 ipa Converts character vectors between phonetic representations. Supports IPA (International Phonetic Alphabet), X-SAMPA (Extended Speech Assessment Methods Phonetic Alphabet), and ARPABET (used by the CMU Pronouncing Dictionary). library(ipa) convert_phonetics(&#39;%hE&quot;loU&#39;, from = &quot;xsampa&quot;, to = &quot;ipa&quot;) [1] &quot;ˌhɛˈloʊ&quot; If you will use the package it make sense to cite it: citation(&quot;ipa&quot;) To cite ipa in publications use: Alexander Rossell Hayes (2020). ipa: convert between phonetic alphabets. R package version 0.1.0. https://github.com/rossellhayes/ipa A BibTeX entry for LaTeX users is @Manual{, title = {ipa: convert between phonetic alphabets}, author = {Rossell Hayes and {Alexander}}, year = {2020}, note = {R package version 0.1.0}, url = {https://github.com/rossellhayes/ipa}, } 5.3 phonTools This package contains tools for the organization, display, and analysis of the sorts of data frequently encountered in phonetics research and experimentation, including the easy creation of IPA vowel plots, and the creation and manipulation of WAVE audio files. This package is usefull, since it provides some datasets: pb52 — Peterson &amp; Barney (1952) Vowel Data (1520 rows) f73 — Fant (1973) Swedish Vowel Data (10 rows) p73 — Pols et al. (1973) Dutch Vowel Data (12 rows) b95 — Bradlow (1995) Spanish Vowel Data (5 rows) h95 — Hillenbrand et al. (1995) Vowel Data (1668 rows) a96 — Aronson et al. (1996) Hebrew Vowel Data (10 rows) y96 — Yang (1996) Korean Vowel Data (20 rows) f99 — Fourakis et al. (1999) Greek Vowel Data (5 rows) t07 — Thomson (2007) Vowel Data (20 rows) In order to load them you need to follow these steps: library(phonTools) data(h95) h95 As you see vowel column is in X-SAMPA. In order to use it we need to recode it: library(tidyverse) h95 %&gt;% mutate(vowel = convert_phonetics(vowel, from = &quot;xsampa&quot;, to = &quot;ipa&quot;)) %&gt;% ggplot(aes(f2, f1, label = vowel, color = vowel))+ stat_ellipse()+ geom_text()+ scale_x_reverse()+ scale_y_reverse()+ coord_fixed()+ facet_wrap(~type) It also make sense to use normalization for vowels: h95 %&gt;% mutate(vowel = convert_phonetics(vowel, from = &quot;xsampa&quot;, to = &quot;ipa&quot;)) %&gt;% group_by(speaker) %&gt;% mutate(f1 = scale(f1), f2 = scale(f2)) %&gt;% ggplot(aes(f2, f1, label = vowel, color = vowel))+ stat_ellipse()+ geom_text()+ scale_x_reverse()+ scale_y_reverse()+ facet_wrap(~type)+ labs(title = &quot;Normalized vowels&quot;) If you will use the package it make sense to cite it: citation(&quot;phonTools&quot;) To cite package &#39;phonTools&#39; in publications use: Santiago Barreda (2015). phonTools: Functions for phonetics in R. R package version 0.2-2.1. A BibTeX entry for LaTeX users is @Manual{, title = {phonTools: Functions for phonetics in R.}, author = {Santiago Barreda}, year = {2015}, note = {R package version 0.2-2.1}, } 5.4 vowels Procedures for the manipulation, normalization, and plotting of phonetic and sociophonetic vowel formant data. vowels is the backend for the NORM website. If you will use the package it make sense to cite it: citation(&quot;vowels&quot;) To cite package &#39;vowels&#39; in publications use: Tyler Kendall and Erik R. Thomas (2018). vowels: Vowel Manipulation, Normalization, and Plotting. R package version 1.2-2. https://CRAN.R-project.org/package=vowels A BibTeX entry for LaTeX users is @Manual{, title = {vowels: Vowel Manipulation, Normalization, and Plotting}, author = {Tyler Kendall and Erik R. Thomas}, year = {2018}, note = {R package version 1.2-2}, url = {https://CRAN.R-project.org/package=vowels}, } ATTENTION: This citation information has been auto-generated from the package DESCRIPTION file and may need manual editing, see &#39;help(&quot;citation&quot;)&#39;. 5.5 rPraat This is a package for reading, writing and manipulating Praat objects like TextGrid, PitchTier, Pitch, IntensityTier, Formant, Sound, and Collection files. You can find the demo for rPraat here. If you will use the package it make sense to cite it: citation(&quot;rPraat&quot;) To cite rPraat in publications use: Bořil, T., &amp; Skarnitzl, R. (2016). Tools rPraat and mPraat. In P. Sojka, A. Horák, I. Kopeček, &amp; K. Pala (Eds.), Text, Speech, and Dialogue (pp. 367-374). Springer International Publishing. A BibTeX entry for LaTeX users is @InProceedings{, author = {Tomáš Bořil and Radek Skarnitzl}, editor = {Petr Sojka and Aleš Horák and Ivan Kopeček and Karel Pala}, booktitle = {Text, Speech, and Dialogue: 19th International Conference, TSD 2016, Brno, Czech Republic, September 12-16, 2016, Proceedings}, publisher = {Springer International Publishing}, address = {Cham}, year = {2016}, title = {Tools rPraat and mPraat}, pages = {367--374}, isbn = {978-3-319-45510-5}, doi = {10.1007/978-3-319-45510-5_42}, url = {http://dx.doi.org/10.1007/978-3-319-45510-5_42}, } 5.6 speakr This package allows running Praat scripts from R and it provides some wrappers for basic plotting. On macOS, Linux and Windows, the path to Praat is set automatically to the default installation path. If you have installed Praat in a different location, or if your operating system is not supported, you can set the path to Praat with options(speakr.praat.path). For example: options(speakr.praat.path = &quot;./custom/praat.exe&quot;) If you use rstudio.cloud you need to upload the unziped file of Linux version of Praat and provide path to it within the options(): options(speakr.praat.path = &quot;/cloud/projects/praat&quot;) You can either run this command every time you start a new R session, or you can add the command to your .Rprofile. If you run into the problem with permisions run the following command: system(&quot;chmod 700 /cloud/projects/praat&quot;) As an example I will use the following data: ├── get-formants-args.praat ├── s1_all.TextGrid └── s1_all.wav Here are the s1_all.TextGrid, s1_all.wav and the get-formants-args.praat: sound = Read from file: &quot;s1_all.wav&quot; formant = To Formant (burg): 0, 5, 5000, 0.05, 50 textgrid = Read from file: &quot;s1_all.TextGrid&quot; header$ = &quot;vowel,F1,F2,F3&quot; writeInfoLine: header$ form Get formants choice Measure 1 button Hertz button Bark real Window_(sec) 0.03 endform if measure == 1 measure$ = &quot;Hertz&quot; else measure$ = &quot;Bark&quot; endif selectObject: textgrid intervals = Get number of intervals: 3 for interval to intervals - 1 label$ = Get label of interval: 3, interval if label$ != &quot;&quot; start = Get start time of interval: 3, interval start = start + window end = Get end time of interval: 3, interval end = end - window vowel$ = Get label of interval: 3, interval selectObject: formant f1 = Get mean: 1, start, end, measure$ f2 = Get mean: 2, start, end, measure$ f3 = Get mean: 3, start, end, measure$ resultLine$ = &quot;&#39;vowel$&#39;,&#39;f1&#39;,&#39;f2&#39;,&#39;f3&#39;&quot; appendInfoLine: resultLine$ selectObject: textgrid endif endfor In order to use it you need to provide path to the script: library(speakr) Praat found at /usr/bin/praat praat_run(&quot;sounds/speakr/get-formants-args.praat&quot;, &quot;Hertz&quot;, 0.03, capture = TRUE) [1] &quot;vowel,F1,F2,F3\\na,832.4811857289797,1530.407389678815,2174.814335788099\\na,775.505720968018,1368.2558211702246,2509.084391503502\\na,833.9754988486553,1430.9906341641104,2375.2400400500196\\na,863.5056920837876,1546.8857016095715,2890.4894773135247\\nə,723.6246579247697,1689.4016617222446,2873.1991954244213\\nə,717.2753390554032,1675.2062036645505,2745.5586414564323\\nə,758.8975215666187,1679.0831544665587,2698.956344303706\\nə,711.4372573263009,1685.1137747815465,3031.5963943557826&quot; As you see the result is file in a .csv format. We can use the following code in order to visualise it: library(tidyverse) praat_run(&quot;sounds/speakr/get-formants-args.praat&quot;, &quot;Hertz&quot;, 0.03, capture = TRUE) %&gt;% read_csv() -&gt; abaza_formants Rows: 8 Columns: 4 ── Column specification ──────────────────────────────────────────────────────── Delimiter: &quot;,&quot; chr (1): vowel dbl (3): F1, F2, F3 ℹ Use `spec()` to retrieve the full column specification for this data. ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. abaza_formants %&gt;% ggplot(aes(F2, F1, label = vowel, color = vowel))+ geom_text()+ scale_x_reverse()+ scale_y_reverse()+ coord_fixed()+ labs(title = &quot;Example of Abaza vowels&quot;) Actually we can merge data on formants and data on duration together: library(phonfieldwork) abaza_duration &lt;- textgrid_to_df(&quot;sounds/speakr/s1_all.TextGrid&quot;) abaza_duration %&gt;% filter(tier == 3, content != &quot;&quot;) %&gt;% mutate(duration = time_end - time_start) %&gt;% select(content, duration) -&gt; abaza_duration abaza_duration abaza_formants %&gt;% bind_cols(abaza_duration) %&gt;% ggplot(aes(F2, F1, label = vowel, color = vowel, size = duration))+ geom_text()+ scale_x_reverse()+ scale_y_reverse()+ coord_fixed()+ labs(title = &quot;Example of Abaza vowels&quot;) If you will use the package it make sense to cite it: citation(&quot;speakr&quot;) To cite package &#39;speakr&#39; in publications use: Stefano Coretta (2021). speakr: A Wrapper for the Phonetic Software &#39;Praat&#39;. R package version 3.2.0. https://CRAN.R-project.org/package=speakr A BibTeX entry for LaTeX users is @Manual{, title = {speakr: A Wrapper for the Phonetic Software &#39;Praat&#39;}, author = {Stefano Coretta}, year = {2021}, note = {R package version 3.2.0}, url = {https://CRAN.R-project.org/package=speakr}, } "],["vowel-formants-normalization.html", "6 Vowel formants’ normalization 6.1 Acoustic vowel normalization procedures 6.2 (Lobanov 1971) z-score transformation 6.3 vowels package", " 6 Vowel formants’ normalization This section is based on (Adank 2003). There are three possible sources of variation in vowel formants measurements (Ladefoged and Broadbent 1957; Pols, Tromp, and Plomp 1973: 1095; Adank 2003): acoustic variation; speaker variation; sociolinguistic; anatomical/physiological variation; and measurement error (“residual variance” in (Pols, Tromp, and Plomp 1973)). There are a lot of researchers aimed to reduce speaker-related variation using acoustic vowel normalization (e. g. (Gerstman 1968; Lobanov 1971; Syrdal and Gopal 1986)). However there are some researches that afraid that normalization procedures can reduce interesting for the linguistics information like sociolinguistic/dialectal signal in data (Hindle 1978; Disner 1980; Thomas 2002, 174–75). Human listeners deal seemingly effortlessly with all three possible sources of variation, but the dataset from (Peterson and Barney 1952) shows extrordinary variation: 6.1 Acoustic vowel normalization procedures There are several classes of vowel normalization procedures: formant-based procedures (Gerstman 1968; Lobanov 1971; Fant 1975; Syrdal and Gopal 1986; Miller 1989); whole-spectrum procedures (Klein, Plomp, and Pols 1970; Pols, Tromp, and Plomp 1973; Bladon and Lindblom 1981; Bladon 1982; Klatt 1982); Neural networks (D. J. M. Weenink 1993; D. Weenink 1997). Formant-based procedures are the most compact (just 2- or 3-dimensional represetations) and comparable crosslinguisticaly. In (Adank 2003) author compared 11 methods of vowel normalization: abb method 1 HZ the baseline condition, formants in Hz 2 LOG a log-transformation of the frequency scale 3 BARK a bark-transformation of the frequency scale 4 MEL a mel-transformation of the frequency scale 5 ERB an ERB-transformation of the frequency scale 6 GERSTMAN Gerstman’s (1968) range normalization 7 LOBANOV Lobanov’s (1971) z-score transformation 8 NORDSTRÖM &amp; LINDBLOM Nordström &amp; Lindblom’s (1975) vocal-tract scaling 9 CLIH i4 Nearey’s (1978) individual log-mean procedure 10 CLIH s4 Nearey’s (1978) shared log-mean procedure 11 SYRDAL &amp; GOPAL Syrdal &amp; Gopal’s (1986) bark-distance model 12 MILLER Miller’s (1989) formant-ratio model 6.2 (Lobanov 1971) z-score transformation The idea behind the Lobanov’s method is simple z-normalization. Imagine some random distribution: If we apply the folowing normalization, the distribution form will be the same, however the scale will be unified with mean = 0 and standard deviation = 1: \\[x_{normalized} = \\frac{x-\\mu}{\\sigma}\\] library(phonTools) data(pb52) pb52 %&gt;% group_by(speaker) %&gt;% mutate(vowel = ipa::convert_phonetics(vowel, from = &quot;xsampa&quot;, to = &quot;ipa&quot;), scaled_f1 = scale(f1), scaled_f2 = scale(f2)) %&gt;% ggplot(aes(scaled_f2, scaled_f1, label = vowel, color = vowel))+ stat_ellipse()+ geom_text()+ scale_x_reverse()+ scale_y_reverse() You can find implementation of other methods in R package vowels. Try to normalize and visualize data from the dataset Hillenbrand et al. (1995), stored in h95 variable in the package phonTools. Sometimes it make sense to get back to the formant values: pb52 %&gt;% mutate(overall_mean_f1 = mean(f1), overall_sd_f1 = sd(f1), overall_mean_f2 = mean(f2), overall_sd_f2 = sd(f2)) %&gt;% group_by(speaker) %&gt;% mutate(vowel = ipa::convert_phonetics(vowel, from = &quot;xsampa&quot;, to = &quot;ipa&quot;), sclaed_f1 = scale(f1), sclaed_f2 = scale(f2), restored_f1 = sclaed_f1*overall_sd_f1+overall_mean_f1, restored_f2 = sclaed_f2*overall_sd_f2+overall_mean_f2) %&gt;% ggplot(aes(restored_f2, restored_f1, label = vowel, color = vowel))+ stat_ellipse()+ geom_text()+ scale_x_reverse()+ scale_y_reverse() 6.3 vowels package You can find implementation of other methods in R package vowels: library(vowels) data(ohiovowels) vowelplot(norm.lobanov(ohiovowels), color=&quot;vowels&quot;, label=&quot;vowels&quot;) vowelplot(norm.labov(ohiovowels), color=&quot;vowels&quot;, label=&quot;vowels&quot;) vowelplot(norm.nearey(ohiovowels), color=&quot;vowels&quot;, label=&quot;vowels&quot;) vowelplot(norm.wattfabricius(ohiovowels), color=&quot;vowels&quot;, label=&quot;vowels&quot;) References "],["sonorants.html", "7 Sonorants 7.1 Earlier: 7.2 Semivowels 7.3 Nasal Stops 7.4 Laterals 7.5 Nasal, nasalised, and prenasalised vowels 7.6 Seminar", " 7 Sonorants 7.1 Earlier: Source-filter model Tube model Movements and positions of the articulators are shaping the resonating cavities of the vocal tract to modify the sound source for resonant/sonorant consonants as well: approximants/glides/semivowels (ʋ, w, j, ɰ, ɥ, /ɹ/?), nasals (m, ɱ, n, ɲ, ŋ, ɴ …), liquids l (lateral) and various /r/ (rhotics: approximants &amp; vibrants = trill + tap / flap). 7.2 Semivowels The vocal tract is relatively open for the semivowels, as it is for vowels. So, the semivowels are characterized acoustically by formants. But they are consonantsǃ Occur on the periphery of syllables, and not in the centers or nuclei of syllables: Russian дай [daj] ‘give’ vs. English die [daɪ] English yo-yo [’jəʊjəʊ] vs. Russian йо-йо [jo’ʝo] But: Russian театр [tʲi’atr] vs [tʲi’atr̥]/[tʲi’atər] ‘theatre’ English button [’bʌtn] Mande languages ŋ ‘I’ 7.2.1 Semivowels: ʋ, w, j, ɥ, ɰ Since articulation of those vowels is really similar to corresponding vowel the spectrum will be simmilar: ʋ, w — u j — i ɰ — ɨ ɥ — y 7.2.2 Semivowels: w, ɥ French Louis [lwi] ‘Louis’ vs. lui [lɥi] ‘him’ 7.2.3 Semivowels: ɹ, ɻ Those semivowels are produced by raising the tongue toward the alveolar ridge, the tip does not touch the alveolar ridge. The acoustic results of these tongue tip adjustments are particularly obvious in the third formant: F3 falls below the F3 frequencies typical of the neighboring vowels. 7.3 Nasal Stops The greater surface area of the vocal tract means that the walls of the vocal tract absorb more energy than in non-nasal sounds, and the greater volume of air means that the inertia of air within the vocal tract absorbs more sound as well. 7.3.1 Nasal Stops: uvular ɴ When the uvula is lowered and the dorsum of the tongue raised to produce an uvular nasal, the vocal tract can be described as a uniform tube that is closed at the glottis and open at the nostrils.If we know the length of the tube, we can calculate its resonant frequencies, because this is a quarter-wave resonator (like the vocal tract configuration for schwa). (Fant 1960b): 12.5 cm (the distance from the uvula to the nares) + 9 cm (the distance from the uvula to the glottis) ≈ 21.5 cm F1 = c/4L = 35000/4*21.5 = 407 Hz F2 = 3c/4L = 1221 Hz F3 = 5c/4L = 2035 Hz F4 = 7c/4L = 2849 Hz Since there is a velocity maximum at the nostrils each of the resonant frequencies will be lower. It is hard to make quantitative predictions about the formant frequencies of ɴ (the shape of the nasal passage varies), but the formant values will be spaced more closely in the uvular nasal than they are in ǝ 7.3.2 Nasal Stops: m 7.3.3 Nasal Stops: anti-resonance The main difference between ɴ and m is that the mouth cavity forms a side branch in the resonant tube. The mouth cavity can be modeled as a tube closed at one end (the lips) and open at the other (the uvula) , with a length of about 9 cm. We can therefore calculate the resonances of the mouth cavity as we did for ǝ, ɴ: c/4L = 35,000 / (4 × 9) = 972 3c/4L = 2,917 Hz The resonant frequencies of the mouth cavity in nasals are not like that we’ve seen before: the mouth cavity is a side branch of a larger resonantor: it doesn’t open directly to the atmosphere. They are “absorbed” in the side branch anti-resonance the frequency components in m that are near the resonant frequencies of the mouth cavity are canceled, and become (anti-formants) in the acoustic output. Formants show up in the spectrum as peaks of sound energy, and anti-formants show up as pronounced spectral valleys. 7.3.4 Nasal Stops: the main properties low F1 (sometimes called the “nasal formant”), close spacing between formants, the presence of anti-formants, whose frequencies are determined by the place of articulation 7.4 Laterals Laterals could be analised in the similar way: a small pocket of air on top of the tongue acts as a 4cm side branch (to the main acoustic channel which curves around one or both sides of the tongue) produce anti-formants in the spectrum (≈ 2,125). 7.5 Nasal, nasalised, and prenasalised vowels The most complicated configuration of the vocal tract: 2 resonant systems at once: the pharynx cavity + the mouth cavity — the oral tract – has resonances at about 500 Hz, 1,500 Hz, and 2,500 Hz; the pharynx cavity + the nasal cavity — the nasal tract – 400, 1,200, 2,000 Hz. All these formants are present in the spectrum of nasal vowels. Of course, the resonant frequencies of the oral tract can be modified by movements of the tongue and lips, constriction of the nose at the nares. For nasalised/prenasalised vowels closed mouth produced antiformants (680 Hz, 2040 Hz). 7.6 Seminar 7.6.1 Glides vs. Vowels Periodic voicing Amplitude lower then in vowels Formants First Formant target is lower than vowels’ First Formant target Open quotient1 (= First Harmonic - Second Harmonic) is lower for glides 7.6.2 Liquids Periodic voicing Amplitude lower then in vowels Formants Wide average spacing of the formants 7.6.3 Nasal Stops Acoustics Periodic voicing Amplitude lower then in vowels Formants Formants have broad bandwidths Low frequency first formant Less space between formants Higher formants have low amplitude Antiformants 7.6.4 Spectral Slices Praat Objects &gt; Open Sound View &amp; Edit Find the middle of nasals or one cycle Spectrum &gt; View Spectrum slice (Ctrl+L) Select Spectrum Slice object in Praat Object window Praat Objects &gt; Draw… We can use the following file with Andic nasals and the following R code for visualisation: setwd(&quot;...&quot;) # put your path here library(tidyverse) n &lt;- read_csv(&quot;n.csv&quot;) n$sound_type &lt;- &quot;n&quot; m &lt;- read_csv(&quot;m.csv&quot;) m$sound_type &lt;- &quot;m&quot; n %&gt;% bind_rows(m) %&gt;% filter(`freq(Hz)` &lt; 5000) %&gt;% ggplot(aes(`freq(Hz)`, `pow(dB/Hz)`, color = sound_type))+ geom_line() References "],["obstruents.html", "8 Obstruents", " 8 Obstruents "],["spectrum-analysis.html", "9 Spectrum analysis", " 9 Spectrum analysis "],["suprasegmentals.html", "10 Suprasegmentals", " 10 Suprasegmentals "],["perception.html", "11 Perception", " 11 Perception "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
